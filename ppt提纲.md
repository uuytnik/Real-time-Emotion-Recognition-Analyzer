## 演示时长建议
- 总时长 8–12 分钟（可按赛制微调）
- 结构比例：创意与价值 20% | Demo 25% | 技术 30% | 体验与伦理 10% | 落地与路线图 10% | Q&A 5%

## 01. 封面
- 标题：实时表情识别分析器（Real-time Emotion Recognition Analyzer）
- 副标题：基于深度学习的人机共情创意系统
- 团队/成员、日期、Logo/主题图
- 一句话介绍：用AI感知情绪，让艺术与交互“懂你所感”

## 02. 创意来源与价值主张
- 痛点：人机交互“无感情”、沉浸式展览缺少个性化反馈、线下活动缺乏实时情绪洞察
- 创意点：让界面/空间对人的情绪“即刻回应”，打造情绪驱动的交互艺术与产品体验
- 价值主张：实时、无侵入、跨场景；提升参与感与记忆度；提供可视化情绪洞察

## 03. 产品功能一览（Feature Map）
- 实时模式：摄像头取流 + 实时推理 + 情绪分布可视化（`/video_feed` + `/emotion`）
- 图片模式：上传图片/粘贴Base64进行批量/单张分析（`/analyze_image`）
- 健康检查：服务状态自检（`/health`）
- 前端交互：视频预览、情绪数据轮询展示、图片上传分析
- 安全限制：文件类型白名单、大小限制、异常处理

## 05. 现场 Demo 流程（含“兜底方案”）
- Demo脚本：
  1) 打开前端页面 frontend.html，展示实时画面 `<img src="/video_feed">`
  2) 触发/展示 `GET /emotion` 的实时情绪分布（轮询/定时拉取）
  3) 上传图片或使用示例图调用 `POST /analyze_image`
- 可视化建议：饼图/条形图显示情绪分布，顶部显示“dominant_emotion”
- 风险与兜底：
  - 摄像头不可用 → 切换到图片模式或播放本地视频模拟流
  - 光照/角度影响识别 → 现场指定最佳站位与补光
  - 性能不稳 → 降分辨率、降低轮询频率或切静态图示例

## 06. 系统整体架构图
- 前端：浏览器页面（`frontend.html`）通过
  - `<img src="/video_feed">` 显示 MJPEG 实时视频
  - 轮询/定时请求 `/emotion` 获取情绪概率
  - 表单或Base64调用 `/analyze_image` 进行图片分析
- 后端：Flask（backend-v1.1.py）
  - OpenCV 采集摄像头帧
  - DeepFace 调用（`DeepFace.analyze(actions=['emotion'])`）完成推理
  - CORS 开启、文件校验、异常处理、健康检查
- 依赖与运行环境：Python、Flask、DeepFace、OpenCV、PIL、NumPy

## 07. 模型与算法选择
- 模型框架：DeepFace（封装多种SOTA表情识别/人脸分析模型，调用便捷）
- 推理流程：
  - BGR → RGB → 模型推理 → 输出情绪概率分布 + 主导情绪
  - 返回字典，保留两位小数，兼容多脸时取第一张/主脸
- 选择理由：开箱即用、社区验证、在创意作品中追求“稳定+快速落地”
- 可能优化：
  - 模型蒸馏/ONNX加速、GPU/CUDA、批量推理
  - 弱光/遮挡鲁棒性增强（预处理、光照补偿、人脸追踪）

## 08. 数据与隐私合规
- 数据来源：本地摄像头帧/上传图片（仅内存处理，不落盘）
- 隐私保护：
  - 不存储原始图像，前端提示与征得同意
  - 数据最小化：仅返回概率分布，不输出人脸特征向量
  - 可选匿名化：实时马赛克/不显示人脸，仅展示情绪图表
- 伦理思考：避免用于敏感场景的自动决策，强调“共情辅助而非评判”

## 09. 后端设计亮点（backend-v1.1.py）
- 路由与职责：
  - `/video_feed`：MJPEG流（生成器按帧编码 JPEG）
  - `/emotion`：从摄像头抓帧实时分析
  - `/analyze_image`：FormData/JSON Base64 双通道输入
  - `/health`：健康状态、版本、摄像头可用
- 健壮性：
  - 文件类型白名单、大小限制（5MB）、Base64粗略体积估算
  - 统一异常捕获与友好错误返回
  - 应用生命周期钩子：启动打印、关闭释放摄像头
- 跨域支持：CORS 全开放（可在生产按域名白名单收敛）
- 可扩展点：异步队列、批处理、缓存最后结果、限流与鉴权

## 10. 前端交互与体验
- 页面结构（frontend.html）：
  - 实时视频区：`<img src="/video_feed">`
  - 情绪图表区：定时请求 `/emotion` 更新图表
  - 图片上传区：`/analyze_image` 返回情绪分布并渲染
- 体验优化建议：
  - 使用 Chart.js/ECharts 展示实时情绪趋势
  - 轻提示与错误态（网络中断/摄像头不可用）
  - 移动端适配与弱网降级（降低轮询频率）

## 11. 性能与稳定性
- 关键指标：端到端延迟、FPS、CPU/GPU占用、内存占用
- 优化清单：
  - 降分辨率与采样率、控制推理帧率
  - 模型前处理并行（I/O 与推理解耦）
  - 可选 GPU/ONNX Runtime 加速
- 压测与监控：简单健康检查 + 控制台日志；后续接入Prometheus/Grafana

## 12. 项目管理与协作
- 代码结构：
  - 后端：backend-v1.1.py、模板/测试脚本
  - 前端：frontend.html
  - 文档：README.md、团队协作规范
- 协作流程：分支策略、提交信息规范、代码评审
- 里程碑：原型 → 可用Demo → 体验优化 → 场景联动 → 试点

## 16. 风险与对策
- 识别误差：光照/角度/遮挡 → 场地布置+预处理+模型调参
- 设备依赖：摄像头/端侧性能 → 兜底静态演示+云端推理备份
- 伦理质疑：明确告知与选择退出，弱化“评判”，强化“共情”

## 17. 路线图（Roadmap）
- 短期：前端图表与交互打磨、设备联动（灯光/音效）
- 中期：ONNX/GPU 加速、多人情绪融合、情绪驱动场景套件
- 长期：多模态（语音、姿态）、个体化学习、可解释性可视化

## 18. 成果总结与号召
- 收益：把“看不见的情绪”变成“可交互的体验”
- 总结金句：让空间与界面“感知情绪、温柔回应”
- 行动：现场体验/试点合作/开源共建

## 19. Q&A 备份页（Appendix）
- API 文档（示例）
  - GET `/video_feed`：MJPEG流
  - GET `/emotion`：返回情绪概率分布（两位小数）
  - POST `/analyze_image`：FormData 文件/JSON Base64，大小≤5MB，扩展名白名单
  - GET `/health`：状态与摄像头可用性
- 错误码与文案：400 参数/文件错误、500 推理失败、413 文件过大
- 性能参数与调优开关：分辨率、推理频率、超时
- 依赖清单与版本（requirements.txt 关键项：Flask、deepface、opencv-python、Pillow、numpy）

---

### 讲稿提示（每页1–2句抓手）
- 创意页：从“人机无感情”小痛点切入，抛出“让空间有共情”的画面感。
- Demo页：先“看见”实时画面，再“看懂”动态情绪分布，最后“可操作”图片上传。
- 技术页：一句话结构图；一句话模型选择理由；一句话隐私承诺。
- 落地页：给出1个最快可落地的试点场景与收费方式。
- Q&A：准备两类问题——技术（精度/延迟/隐私）与应用（误用/伦理/商业化）。

### 评审关注点对齐
- 创意性：情绪→交互/装置/内容驱动的“共情体验”
- 技术深度：端到端实现、稳定性与优化路线图
- 可用性与演示力：实时运行、可视化清晰、兜底方案齐全
- 价值与影响：明确的场景与商业化路径
- 合规与伦理：隐私最小化与正向价值导向

如果你需要，我可以把这个提纲快速填充成可直接演示的PPT母版（含占位图、图标和“Demo脚本”备注），或根据你的答辩时长做精简版（例如 10 页极速版）。